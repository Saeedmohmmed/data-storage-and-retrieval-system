{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import * \n",
    "import re\n",
    "import os \n",
    "import string \n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to clean text \n",
    "def text_to_wordlist(text, remove_stop_words=True, stem_words=False):\n",
    "    # Clean the text, with the option to remove stop_words and to stem words.\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"\", text)\n",
    "    text = re.sub(r\"What's\", \"\", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"I'm\", \"I am\", text)\n",
    "    text = re.sub(r\" m \", \" am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"60k\", \" 60000 \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e-mail\", \"email\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"quikly\", \"quickly\", text)\n",
    "    text = re.sub(r\" usa \", \" America \", text)\n",
    "    text = re.sub(r\" USA \", \" America \", text)\n",
    "    text = re.sub(r\" u s \", \" America \", text)\n",
    "    text = re.sub(r\" uk \", \" England \", text)\n",
    "    text = re.sub(r\" UK \", \" England \", text)\n",
    "    text = re.sub(r\"india\", \"India\", text)\n",
    "    text = re.sub(r\"switzerland\", \"Switzerland\", text)\n",
    "    text = re.sub(r\"china\", \"China\", text)\n",
    "    text = re.sub(r\"chinese\", \"Chinese\", text) \n",
    "    text = re.sub(r\"imrovement\", \"improvement\", text)\n",
    "    text = re.sub(r\"intially\", \"initially\", text)\n",
    "    text = re.sub(r\"quora\", \"Quora\", text)\n",
    "    text = re.sub(r\" dms \", \"direct messages \", text)  \n",
    "    text = re.sub(r\"demonitization\", \"demonetization\", text) \n",
    "    text = re.sub(r\"actived\", \"active\", text)\n",
    "    text = re.sub(r\"kms\", \" kilometers \", text)\n",
    "    text = re.sub(r\"KMs\", \" kilometers \", text)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text) \n",
    "    text = re.sub(r\" upvotes \", \" up votes \", text)\n",
    "    text = re.sub(r\" iPhone \", \" phone \", text)\n",
    "    text = re.sub(r\"\\0rs \", \" rs \", text) \n",
    "    text = re.sub(r\"calender\", \"calendar\", text)\n",
    "    text = re.sub(r\"ios\", \"operating system\", text)\n",
    "    text = re.sub(r\"gps\", \"GPS\", text)\n",
    "    text = re.sub(r\"gst\", \"GST\", text)\n",
    "    text = re.sub(r\"programing\", \"programming\", text)\n",
    "    text = re.sub(r\"bestfriend\", \"best friend\", text)\n",
    "    text = re.sub(r\"dna\", \"DNA\", text)\n",
    "    text = re.sub(r\"III\", \"3\", text) \n",
    "    text = re.sub(r\"the US\", \"America\", text)\n",
    "    text = re.sub(r\"Astrology\", \"astrology\", text)\n",
    "    text = re.sub(r\"Method\", \"method\", text)\n",
    "    text = re.sub(r\"Find\", \"find\", text) \n",
    "    text = re.sub(r\"banglore\", \"Banglore\", text)\n",
    "    text = re.sub(r\" J K \", \" JK \", text)\n",
    "    \n",
    "    # Remove punctuation from text\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    '''\n",
    "    if remove_stop_words:\n",
    "        text = text.split()\n",
    "        text = [w for w in text if not w in stopwords]\n",
    "        text = \" \".join(text)\n",
    "     '''\n",
    "    # Optionally, shorten words to their stems\n",
    "    '''\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "   '''  \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to read files\n",
    "def read_first_line(file):\n",
    "     with open(file, 'rt') as fd: #read text\n",
    "        first_line = fd.readline()\n",
    "        return first_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc1 : ['system', 'system', 'saeed', 'mohmmed']\n",
      "Doc2 : ['computer', 'science', 'engineering']\n",
      "Doc3 : ['system', 'system', 'saeed']\n",
      "Doc4 : ['system', 'system', 'syStem', 'information', 'saeed']\n",
      "Doc5 : ['syStem', 'information']\n",
      "Doc6 : ['system', 'information']\n",
      "Doc7 : ['information', 'security']\n",
      "Doc8 : ['information', 'security', 'system']\n",
      "Doc9 : ['copy', 'information', 'security']\n",
      "Doc10 : ['computer', 'science']\n"
     ]
    }
   ],
   "source": [
    "#------------------tokenization and stop word \n",
    "dict={'Doc1':[],'Doc2':[],'Doc3':[],'Doc4':[],'Doc5':[],'Doc6':[],'Doc7':[],'Doc8':[],'Doc9':[],'Doc10':[]}\n",
    "xlist=[]\n",
    "base_path =os.listdir(\"C://Users//20100//Documents//project_ir/New folder/\")\n",
    "for file in base_path:\n",
    "    if file.endswith('.txt'):\n",
    "        file_text_reader=read_first_line(file) \n",
    "        cleaned=text_to_wordlist(file_text_reader) \n",
    "        text_tokens=word_tokenize(cleaned)\n",
    "        tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "        xlist.append(tokens_without_sw)\n",
    "        \n",
    "        #print(tokens_without_sw) \n",
    "        \n",
    "            \n",
    "#dict \n",
    "#xlist \n",
    "#for x in xlist:\n",
    "dict['Doc1']=xlist[0]\n",
    "dict['Doc2']=xlist[1]\n",
    "dict['Doc3']=xlist[2]\n",
    "dict['Doc4']=xlist[3]\n",
    "dict['Doc5']=xlist[4]\n",
    "dict['Doc6']=xlist[5]\n",
    "dict['Doc7']=xlist[6]\n",
    "dict['Doc8']=xlist[7]\n",
    "dict['Doc9']=xlist[8]\n",
    "dict['Doc10']=xlist[9]\n",
    "#dict\n",
    "#dict['Doc1']\n",
    "for key in list(dict.keys()): \n",
    "    print(key, \":\", dict[key]) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constructing Auxiliary structure(s)\n",
    "uniqueWordsPerDoc = []\n",
    "numWordPerDoc = []\n",
    "totalWords = 0\n",
    "for document in xlist:\n",
    "    num = len(document)\n",
    "    totalWords += num\n",
    "    numWordPerDoc.append(num)\n",
    "    list_set  = set(document)\n",
    "    uniqueWordsPerDoc += (list(list_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number Of word is:  29\n",
      "Number of unique Words is:  10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Creat unique Words in all Documents\n",
    "list_set  = set(uniqueWordsPerDoc)\n",
    "uniqueWords = (list(list_set))\n",
    "\n",
    "print(\"total number Of word is: \",totalWords)\n",
    "print(\"Number of unique Words is: \",len(uniqueWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'saeed': {0: [2], 2: [2], 3: [4]},\n",
       " 'security': {6: [1], 7: [1], 8: [2]},\n",
       " 'mohmmed': {0: [3]},\n",
       " 'science': {1: [1], 9: [1]},\n",
       " 'computer': {1: [0], 9: [0]},\n",
       " 'information': {3: [3], 4: [1], 5: [1], 6: [0], 7: [0], 8: [1]},\n",
       " 'syStem': {3: [2], 4: [0]},\n",
       " 'copy': {8: [0]},\n",
       " 'engineering': {1: [2]},\n",
       " 'system': {0: [0, 1], 2: [0, 1], 3: [0, 1], 5: [0], 7: [2]}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "words = {}\n",
    "for word in uniqueWords:\n",
    "    words[word] = {}\n",
    "    docNum = 0\n",
    "    for document in xlist:\n",
    "        if word in document:\n",
    "            index_pos_list = [ i for i in range(len(document)) if document[i] == word ]\n",
    "            words[word][docNum] = index_pos_list\n",
    "        docNum += 1\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({3: [[2], [3]], 4: [[0], [1]]}, {})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "def phrase_query(query):\n",
    "    queryList = regexp_tokenize(query, \"[\\w']+\")\n",
    "    documentList = []\n",
    "    flag = 0\n",
    "    # get Posting List for  evry Word \n",
    "    for word in queryList:\n",
    "        try:\n",
    "            documentList.append(list(words[word].keys()))\n",
    "        except:\n",
    "            return -1\n",
    "        \n",
    "    # get intersection Documents between words\n",
    "    intersectDocs = documentList[0]\n",
    "    for i in range(1,len(documentList)):\n",
    "        intersectDocs =  list( set(intersectDocs).intersection(documentList[i]) )\n",
    "        flag = 1\n",
    "    \n",
    "     \n",
    "    if flag == 1:\n",
    "        # Validate indexing\n",
    "        query_intersection = {}\n",
    "        query_Notintersection = {}\n",
    "        for Docnum in intersectDocs:\n",
    "            indexsList = []\n",
    "            for word in queryList :\n",
    "                indexsList.append(words[word][Docnum] )\n",
    "            for i in range(1,len(indexsList)): # change range(1,len(intersectIndex)) with k \n",
    "                indexsList[i] = list(np.asarray(indexsList[i]) - i)\n",
    "                \n",
    "            # get intersection Index between Words \n",
    "            intersectIndexs = indexsList[0]\n",
    "            for i in range(1,len(indexsList)):\n",
    "                intersectIndexs = list( set(intersectIndexs).intersection(indexsList[i]) )\n",
    "                \n",
    "            # save intersections \n",
    "            if len(intersectIndexs) > 0:\n",
    "                query_intersection[Docnum]=[]\n",
    "                for i in range(0,len(queryList)): # change range(1,len(queryList)) with k \n",
    "                    query_intersection[Docnum].append(list(np.asarray(intersectIndexs)+i))\n",
    "            else:\n",
    "                query_Notintersection[Docnum]=[]\n",
    "                for i in range(0,len(indexsList)): # change range(1,len(queryList)) with k\n",
    "                    query_Notintersection[Docnum].append(indexsList[i])\n",
    "        return query_intersection,query_Notintersection\n",
    "                               \n",
    "    else:\n",
    "        return intersectDocs\n",
    "    \n",
    "phrase_query(\"syStem information\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<saeed 3\n",
      "   0  :  [2]\n",
      "   2  :  [2]\n",
      "   3  :  [4]\n"
     ]
    }
   ],
   "source": [
    "term = \"saeed\"\n",
    "print(\"<{} {}\".format(term,len(words[term])))\n",
    "for key, value in words[term].items():\n",
    "    print('  ',key, ' : ', value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------term frequency \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------idf \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----tf-idf \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------similsrity between query and document--- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>saeed</th>\n",
       "      <th>mohmmed</th>\n",
       "      <th>system</th>\n",
       "      <th>engineering</th>\n",
       "      <th>science</th>\n",
       "      <th>computer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      saeed  mohmmed  system  engineering  science  computer\n",
       "doc0      1        1       2            0        0         0\n",
       "doc1      0        0       0            1        1         1\n",
       "doc2      1        0       2            0        0         0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#split so each word have their own string\n",
    "\n",
    "total= set(xlist[0]).union(xlist[1]).union(xlist[2])\n",
    "#print(total)\n",
    "wordDictA = dict.fromkeys(total, 0) \n",
    "wordDictB = dict.fromkeys(total, 0)\n",
    "wordDictc = dict.fromkeys(total, 0)\n",
    "for word in xlist[0]:\n",
    "    wordDictA[word]+=1\n",
    "    \n",
    "for word in  xlist[1]:\n",
    "    wordDictB[word]+=1\n",
    "    \n",
    "for word in  xlist[2]:\n",
    "    wordDictc[word]+=1  \n",
    "rownames = ['doc0','doc1','doc2']    \n",
    "pd.DataFrame([wordDictA, wordDictB,wordDictc],index=rownames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>saeed</th>\n",
       "      <th>mohmmed</th>\n",
       "      <th>system</th>\n",
       "      <th>engineering</th>\n",
       "      <th>science</th>\n",
       "      <th>computer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc0</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         saeed  mohmmed    system  engineering   science  computer\n",
       "doc0  0.250000     0.25  0.500000     0.000000  0.000000  0.000000\n",
       "doc1  0.000000     0.00  0.000000     0.333333  0.333333  0.333333\n",
       "doc2  0.333333     0.00  0.666667     0.000000  0.000000  0.000000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeTF(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(corpusCount)\n",
    "    return(tfDict)\n",
    "tfFirst = computeTF(wordDictA, xlist[0])\n",
    "tfSecond = computeTF(wordDictB,xlist[1]) \n",
    "tfthird = computeTF(wordDictc,xlist[2])\n",
    "tf = pd.DataFrame([tfFirst, tfSecond,tfthird],index=rownames)\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'saeed': 0.47712125471966244,\n",
       " 'mohmmed': 0.47712125471966244,\n",
       " 'system': 0.47712125471966244,\n",
       " 'engineering': 0.47712125471966244,\n",
       " 'science': 0.47712125471966244,\n",
       " 'computer': 0.47712125471966244}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / (float(val) + 1))\n",
    "    return(idfDict)\n",
    "\n",
    "idfs = computeIDF([wordDictA, wordDictB,wordDictc]) \n",
    "idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>saeed</th>\n",
       "      <th>mohmmed</th>\n",
       "      <th>system</th>\n",
       "      <th>engineering</th>\n",
       "      <th>science</th>\n",
       "      <th>computer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc0</th>\n",
       "      <td>0.11928</td>\n",
       "      <td>0.11928</td>\n",
       "      <td>0.238561</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.15904</td>\n",
       "      <td>0.15904</td>\n",
       "      <td>0.15904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>0.15904</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.318081</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        saeed  mohmmed    system  engineering  science  computer\n",
       "doc0  0.11928  0.11928  0.238561      0.00000  0.00000   0.00000\n",
       "doc1  0.00000  0.00000  0.000000      0.15904  0.15904   0.15904\n",
       "doc2  0.15904  0.00000  0.318081      0.00000  0.00000   0.00000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return(tfidf)\n",
    "#running our two sentences through the IDF:\n",
    "idfFirst = computeTFIDF(tfFirst, idfs)\n",
    "idfSecond = computeTFIDF(tfSecond, idfs)\n",
    "idfthird = computeTFIDF(tfthird, idfs)\n",
    "#putting it in a dataframe\n",
    "idf= pd.DataFrame([idfFirst, idfSecond,idfthird],index=rownames) \n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
